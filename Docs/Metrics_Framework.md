# 产品评估指标框架 (Metrics Framework) - "环应急战"教学平台

## 1. 指标框架概述

本框架旨在建立一套科学、可量化的指标体系，用于评估“环应急战”教学平台在**产品应用**、**用户参与**和**教学效果**三个维度的表现。指标的设定将遵循SMART原则（具体、可衡量、可实现、相关、有时限），为产品的迭代优化、效果评估和价值展示提供客观的数据支持。

## 2. 北极星指标 (North Star Metric)

* **定义**: **有效推演总时长 (Total Effective Engagement Time)**
* **计算方式**: `(成功完成的课堂推演场次) x (平均每场推演时长) x (平均参与学生比例)`
* **选择依据**: 这个指标综合反映了产品的核心价值。
    * **教师的采纳度**: 体现在“推演场次”上，教师使用得越多，场次越多。
    * **学生的参与度**: 体现在“平均参与学生比例”上，越吸引人，参与比例越高。
    * **产品的粘性**: 体现在“平均推演时长”上，内容越丰富、流程越顺畅，学生和老师投入的时间越长。
    * 最终，有效投入的时间越长，意味着产品在辅助教学、传递知识方面的作用越大，是衡量产品是否成功的最终体现。

## 3. 指标体系详述 (HEART / AARRR 变体)

我们将结合主流的指标模型，构建一个更贴合教学软件特点的评估体系。

### **A. 采纳度 (Adoption)** - 衡量产品被使用的广度

| 指标名称         | 定义与计算                                  | 数据来源     | 监测频率 | 目标 (V1.0)              |
| :--------------- | :------------------------------------------ | :----------- | :------- | :----------------------- |
| **教师激活率** | (首次登录并创建推演的教师数 / 总教师数) x 100% | 后台日志     | 每学期   | > 80% (目标教师全部激活) |
| **课堂渗透率** | (使用本产品授课的堂数 / 《应急监测》总堂数) x 100% | 教师访谈/后台 | 每月     | > 30%                    |
| **学生覆盖率** | (参与过至少一次推演的学生数 / 总学生数) x 100% | 后台日志     | 每月     | > 90%                    |

### **B. 参与度 (Engagement)** - 衡量用户使用的深度和频率

| 指标名称               | 定义与计算                                      | 数据来源     | 监测频率 | 目标 (V1.0)            |
| :--------------------- | :---------------------------------------------- | :----------- | :------- | :--------------------- |
| **平均推演时长** | 单次推演从开始到结束的平均时间                  | 后台日志     | 每周     | 45-60 分钟             |
| **核心功能使用率** | (使用过核心决策功能的小组数 / 总小组数) x 100%    | 后台日志     | 每场     | > 95%                  |
| **对抗模式使用比例** | (选择对抗模式的场次 / 总场次) x 100%              | 后台日志     | 每月     | > 70% (验证游戏化吸引力) |

### **C. 教学有效性 (Efficacy)** - 衡量产品对教学目标的贡献 (核心)

| 指标名称             | 定义与计算                                                                             | 数据来源                 | 监测频率 | 目标 (V1.0)                    |
| :------------------- | :------------------------------------------------------------------------------------- | :----------------------- | :------- | :----------------------------- |
| **知识点掌握度提升** | (推演后测验平均分 - 推演前测验平均分) / 推演前测验平均分 x 100%                          | 课前/课后小测验 (线下) | 每案例  | 提升率 > 20%                   |
| **决策能力评分** | 系统根据学生在推演中的决策（布点、定项目等）与最优解的贴近度，自动计算出的平均分。     | 后台算法                 | 每场     | 追踪分数趋势，期望持续提升     |
| **教师主观评价 (NPS)** | 通过问卷询问教师向同事推荐本产品的意愿度 (0-10分)。                                    | 问卷调查                 | 每学期   | NPS > 30                       |
| **学生满意度** | 通过问卷询问学生对推演课堂的喜爱程度 (1-5分)。                                         | 问卷调查                 | 每学期   | 平均分 > 4.0                   |

### **D. 留存率 (Retention)** - 衡量产品的持续吸引力

| 指标名称         | 定义与计算                                        | 数据来源 | 监测频率 | 目标 (V1.0)    |
| :--------------- | :------------------------------------------------ | :------- | :------- | :------------- |
| **教师周留存率** | (本周至少开课1次的教师 / 上周至少开课1次的教师) x 100% | 后台日志 | 每周     | 保持稳定       |

## 4. 功能级评估指标

| 功能模块     | 关键评估指标                                 | 目标                                         |
| :----------- | :------------------------------------------- | :------------------------------------------- |
| **案例推演** | **任务完成率**: (完成所有阶段的小组 / 总小组) x 100% | > 90%，若过低说明难度或流程设计有问题。     |
| **数据复盘** | **复盘功能使用率**: (使用过决策对比的教师 / 总教师) x 100% | > 60%，验证该功能的实用性。                 |
| **理论知识库** | **页面点击率**: 知识库页面在推演或复盘过程中的点击次数 | 追踪点击变化，评估其作为辅助工具的价值。     |

## 5. 指标监测计划

* **数据收集**:
    * **前端埋点/后端日志**: 负责收集所有线上行为数据 (Adoption, Engagement, Retention)。
    * **问卷调查**: 在期中/期末，通过问卷星等工具向师生定向发放，收集教学有效性中的主观评价数据。
    * **线下测验**: 由教师配合，在进行特定案例推演课的前后，对学生进行简短的知识点测验，用于评估教学有效性。
* **数据报告**:
    * **产品周报**: 监控核心的应用和参与度指标，发现异常波动。
    * **版本/季度报告**: 对一个完整版本或学期的所有指标进行汇总分析，评估产品整体表现，并为下一个版本的规划提供数据输入。
* **责任人**: 产品经理 (1.产品经理) 负责指标体系的维护、数据的分析解读，并向项目组同步报告。

---